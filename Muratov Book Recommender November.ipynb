{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alexander Muratov's Book Recommdner\n",
    "\n",
    "I have often wondered why Goodreads, Amazon, and Audible just don't really do a good job in figuring out which books to recommend to me. Perhaps books are a little too personal and situational compared to movies and music. You invest much more time, and often can't boil down your review to just a number. How can you put a work of classic literature alongside a fun comic book? \n",
    "\n",
    "But I decided that I would give it a shot anyway. I decided to try the collaborative filtering algorithm I learned in Andrew Ng's coursera Machine Learning course on a public book rating dataset, and see what I get. Perhaps it would give me some cool ideas for what I should be reading, or at least help me see the challenges in solving this problem.\n",
    "\n",
    "\n",
    "## Using data from  Book-Crossing Dataset \n",
    "http://www2.informatik.uni-freiburg.de/~cziegler/BX/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1. Data Cleaning \n",
    "Here, I am going to take the original raw data files and pull out only the information I need to build my recommender system.\n",
    "The code presented here is a significantly streamlined version of the first time I actually tried to explore this data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#first CSV file gives ratings for users for books by code number, but no titles\n",
    "data = pd.read_csv(\"BookData_A/BX-Book-Ratings.csv\", encoding='latin-1', sep=';')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 6452: expected 8 fields, saw 9\\nSkipping line 43667: expected 8 fields, saw 10\\nSkipping line 51751: expected 8 fields, saw 9\\n'\n",
      "b'Skipping line 92038: expected 8 fields, saw 9\\nSkipping line 104319: expected 8 fields, saw 9\\nSkipping line 121768: expected 8 fields, saw 9\\n'\n",
      "b'Skipping line 144058: expected 8 fields, saw 9\\nSkipping line 150789: expected 8 fields, saw 9\\nSkipping line 157128: expected 8 fields, saw 9\\nSkipping line 180189: expected 8 fields, saw 9\\nSkipping line 185738: expected 8 fields, saw 9\\n'\n",
      "b'Skipping line 209388: expected 8 fields, saw 9\\nSkipping line 220626: expected 8 fields, saw 9\\nSkipping line 227933: expected 8 fields, saw 11\\nSkipping line 228957: expected 8 fields, saw 10\\nSkipping line 245933: expected 8 fields, saw 9\\nSkipping line 251296: expected 8 fields, saw 9\\nSkipping line 259941: expected 8 fields, saw 9\\nSkipping line 261529: expected 8 fields, saw 9\\n'\n",
      "C:\\Users\\Alejandro\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2723: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "#second CSV file gives book titles. Some of the titles have weird characters, so it must be read with latin-1 encoding\n",
    "#This was a painful thing to figure out. \n",
    "bookdata = pd.read_csv(\"BookData_A/BX-Books.csv\", encoding='latin-1', sep=';',error_bad_lines=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User-ID</th>\n",
       "      <th>ISBN</th>\n",
       "      <th>Book-Rating</th>\n",
       "      <th>Book-Title</th>\n",
       "      <th>Book-Author</th>\n",
       "      <th>Year-Of-Publication</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>Image-URL-S</th>\n",
       "      <th>Image-URL-M</th>\n",
       "      <th>Image-URL-L</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>276726</td>\n",
       "      <td>0155061224</td>\n",
       "      <td>5</td>\n",
       "      <td>Rites of Passage</td>\n",
       "      <td>Judith Rae</td>\n",
       "      <td>2001</td>\n",
       "      <td>Heinle</td>\n",
       "      <td>http://images.amazon.com/images/P/0155061224.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0155061224.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0155061224.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>276729</td>\n",
       "      <td>052165615X</td>\n",
       "      <td>3</td>\n",
       "      <td>Help!: Level 1</td>\n",
       "      <td>Philip Prowse</td>\n",
       "      <td>1999</td>\n",
       "      <td>Cambridge University Press</td>\n",
       "      <td>http://images.amazon.com/images/P/052165615X.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/052165615X.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/052165615X.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>276729</td>\n",
       "      <td>0521795028</td>\n",
       "      <td>6</td>\n",
       "      <td>The Amsterdam Connection : Level 4 (Cambridge ...</td>\n",
       "      <td>Sue Leather</td>\n",
       "      <td>2001</td>\n",
       "      <td>Cambridge University Press</td>\n",
       "      <td>http://images.amazon.com/images/P/0521795028.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0521795028.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0521795028.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>276744</td>\n",
       "      <td>038550120X</td>\n",
       "      <td>7</td>\n",
       "      <td>A Painted House</td>\n",
       "      <td>JOHN GRISHAM</td>\n",
       "      <td>2001</td>\n",
       "      <td>Doubleday</td>\n",
       "      <td>http://images.amazon.com/images/P/038550120X.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/038550120X.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/038550120X.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>276747</td>\n",
       "      <td>0060517794</td>\n",
       "      <td>9</td>\n",
       "      <td>Little Altars Everywhere</td>\n",
       "      <td>Rebecca Wells</td>\n",
       "      <td>2003</td>\n",
       "      <td>HarperTorch</td>\n",
       "      <td>http://images.amazon.com/images/P/0060517794.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0060517794.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0060517794.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    User-ID        ISBN  Book-Rating  \\\n",
       "1    276726  0155061224            5   \n",
       "3    276729  052165615X            3   \n",
       "4    276729  0521795028            6   \n",
       "8    276744  038550120X            7   \n",
       "16   276747  0060517794            9   \n",
       "\n",
       "                                           Book-Title    Book-Author  \\\n",
       "1                                    Rites of Passage     Judith Rae   \n",
       "3                                      Help!: Level 1  Philip Prowse   \n",
       "4   The Amsterdam Connection : Level 4 (Cambridge ...    Sue Leather   \n",
       "8                                     A Painted House   JOHN GRISHAM   \n",
       "16                           Little Altars Everywhere  Rebecca Wells   \n",
       "\n",
       "   Year-Of-Publication                   Publisher  \\\n",
       "1                 2001                      Heinle   \n",
       "3                 1999  Cambridge University Press   \n",
       "4                 2001  Cambridge University Press   \n",
       "8                 2001                   Doubleday   \n",
       "16                2003                 HarperTorch   \n",
       "\n",
       "                                          Image-URL-S  \\\n",
       "1   http://images.amazon.com/images/P/0155061224.0...   \n",
       "3   http://images.amazon.com/images/P/052165615X.0...   \n",
       "4   http://images.amazon.com/images/P/0521795028.0...   \n",
       "8   http://images.amazon.com/images/P/038550120X.0...   \n",
       "16  http://images.amazon.com/images/P/0060517794.0...   \n",
       "\n",
       "                                          Image-URL-M  \\\n",
       "1   http://images.amazon.com/images/P/0155061224.0...   \n",
       "3   http://images.amazon.com/images/P/052165615X.0...   \n",
       "4   http://images.amazon.com/images/P/0521795028.0...   \n",
       "8   http://images.amazon.com/images/P/038550120X.0...   \n",
       "16  http://images.amazon.com/images/P/0060517794.0...   \n",
       "\n",
       "                                          Image-URL-L  \n",
       "1   http://images.amazon.com/images/P/0155061224.0...  \n",
       "3   http://images.amazon.com/images/P/052165615X.0...  \n",
       "4   http://images.amazon.com/images/P/0521795028.0...  \n",
       "8   http://images.amazon.com/images/P/038550120X.0...  \n",
       "16  http://images.amazon.com/images/P/0060517794.0...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Merging the two CSV files to get a catalog with book titles and ratings \n",
    "catalog = data.merge(bookdata, on='ISBN', how='left')\n",
    "\n",
    "#Getting rid of non-reviews - entries with a rating of 0\n",
    "#also getting rid of book titles that are NULL value.\n",
    "reduced_catalog = catalog[(catalog['Book-Rating']>0) & (catalog['Book-Title'].notnull())]\n",
    "\n",
    "\n",
    "#check to make sure catalog looks ok\n",
    "reduced_catalog.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['The Lovely Bones: A Novel', 'Wild Animus', 'The Da Vinci Code',\n",
       "       'The Secret Life of Bees', 'The Nanny Diaries: A Novel'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mycounts = reduced_catalog[\"Book-Title\"].value_counts()\n",
    "mycounts.index[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['The Lovely Bones: A Novel', 'Wild Animus', 'The Da Vinci Code',\n",
       "       'The Secret Life of Bees', 'The Nanny Diaries: A Novel'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#looks good. Now lets figure out which books are the ones that are reviewed most often to feed to the recommender system \n",
    "\n",
    "#lets start with a catalog of 1000 most reviewed books\n",
    "num_top_books = 1000\n",
    "\n",
    "countsorted_reviewed_books =  reduced_catalog[\"Book-Title\"].value_counts()\n",
    "most_reviewedbooks = countsorted_reviewed_books.index[0:num_top_books]\n",
    "most_reviewed_bookcounts = countsorted_reviewed_books.values[0:num_top_books]\n",
    "most_reviewedbooks[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([707, 581, 494, 406, 393], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_reviewed_bookcounts[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Here on Earth (Oprah's Book Club)', 'Over the Edge',\n",
       "       'Breakfast of Champions',\n",
       "       'The Time Traveler's Wife (Today Show Book Club #15)',\n",
       "       'House Atreides (Dune: House Trilogy, Book 1)',\n",
       "       'The Magician's Nephew (rack) (Narnia)', 'Love You Forever',\n",
       "       'Reap the Wind', 'Revenge of the Middle-Aged Woman',\n",
       "       'The Temple of My Familiar', 'Mistaken Identity',\n",
       "       'The Cat Who Brought Down the House', 'The Edge',\n",
       "       'Skinwalkers (Joe Leaphorn/Jim Chee Novels)', 'Bookends : A Novel',\n",
       "       'Uh-Oh: Some Observations from Both Sides of the Refrigerator Door',\n",
       "       'Their Eyes Were Watching God', 'The Tortilla Curtain',\n",
       "       'Fear and Loathing in Las Vegas : A Savage Journey to the Heart of the American Dream',\n",
       "       'Lake Wobegon Days', 'Russendisko.', 'Mind Prey', 'The Midnight Club',\n",
       "       'Moment of Truth'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets also make sure that the ones at the bottom of the list have enough reviews\n",
    "most_reviewedbooks[-25:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36], dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_reviewed_bookcounts[-15:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#not bad. Seems like it will be a robust list\n",
    "#because I am still not totally used to working with dataframes and my algorithm is designed for simple matrices and arrays\n",
    "#I am going to start migrating some stuff to arrays now\n",
    "titles = np.array( most_reviewedbooks)\n",
    "\n",
    "#if someone wants to review the books themselves, uncomment to save text file\n",
    "#np.savetxt(\"top_book_titles.txt\",titles, fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#reducing the dataframe to only contain data for top X books\n",
    "\n",
    "further_reduced_catalogCut = reduced_catalog[\"Book-Title\"].isin(titles)\n",
    "further_reduced_catalog = reduced_catalog[further_reduced_catalogCut]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1270,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now lets get a set of the most active reviewers \n",
    "\n",
    "reviewingusers =  further_reduced_catalog[\"User-ID\"].value_counts()\n",
    "\n",
    "#i don't want any reviewers with fewer than 10 reviews.\n",
    "top_users = reviewingusers[reviewingusers.values>10]\n",
    "book_reviewers = np.array(top_users.index)\n",
    "book_reviewers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#I'm going to subddivide users into a test set and training set.\n",
    "#as of now I am not actually doing this step, but if I wanted  to validate the recommender system,\n",
    "#I would use only 70% of the data t otrain the model, and the other 30% to test. \n",
    "use_set = np.random.rand(top_users.shape[0]) > 0.7 \n",
    "test_set = np.invert(use_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User-ID</th>\n",
       "      <th>ISBN</th>\n",
       "      <th>Book-Rating</th>\n",
       "      <th>Book-Title</th>\n",
       "      <th>Book-Author</th>\n",
       "      <th>Year-Of-Publication</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>Image-URL-S</th>\n",
       "      <th>Image-URL-M</th>\n",
       "      <th>Image-URL-L</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>277427</td>\n",
       "      <td>002542730X</td>\n",
       "      <td>10</td>\n",
       "      <td>Politically Correct Bedtime Stories: Modern Ta...</td>\n",
       "      <td>James Finn Garner</td>\n",
       "      <td>1994</td>\n",
       "      <td>John Wiley &amp;amp; Sons Inc</td>\n",
       "      <td>http://images.amazon.com/images/P/002542730X.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/002542730X.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/002542730X.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1474</th>\n",
       "      <td>277427</td>\n",
       "      <td>0061009059</td>\n",
       "      <td>9</td>\n",
       "      <td>One for the Money (Stephanie Plum Novels (Pape...</td>\n",
       "      <td>Janet Evanovich</td>\n",
       "      <td>1995</td>\n",
       "      <td>HarperTorch</td>\n",
       "      <td>http://images.amazon.com/images/P/0061009059.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0061009059.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0061009059.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1522</th>\n",
       "      <td>277427</td>\n",
       "      <td>0316776963</td>\n",
       "      <td>8</td>\n",
       "      <td>Me Talk Pretty One Day</td>\n",
       "      <td>David Sedaris</td>\n",
       "      <td>2001</td>\n",
       "      <td>Back Bay Books</td>\n",
       "      <td>http://images.amazon.com/images/P/0316776963.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0316776963.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0316776963.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1543</th>\n",
       "      <td>277427</td>\n",
       "      <td>0345413903</td>\n",
       "      <td>10</td>\n",
       "      <td>The Murder Book</td>\n",
       "      <td>Jonathan Kellerman</td>\n",
       "      <td>2003</td>\n",
       "      <td>Ballantine Books</td>\n",
       "      <td>http://images.amazon.com/images/P/0345413903.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0345413903.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0345413903.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578</th>\n",
       "      <td>277427</td>\n",
       "      <td>0385424736</td>\n",
       "      <td>9</td>\n",
       "      <td>The Rainmaker</td>\n",
       "      <td>John Grisham</td>\n",
       "      <td>1995</td>\n",
       "      <td>Doubleday Books</td>\n",
       "      <td>http://images.amazon.com/images/P/0385424736.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0385424736.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0385424736.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      User-ID        ISBN  Book-Rating  \\\n",
       "1456   277427  002542730X           10   \n",
       "1474   277427  0061009059            9   \n",
       "1522   277427  0316776963            8   \n",
       "1543   277427  0345413903           10   \n",
       "1578   277427  0385424736            9   \n",
       "\n",
       "                                             Book-Title         Book-Author  \\\n",
       "1456  Politically Correct Bedtime Stories: Modern Ta...   James Finn Garner   \n",
       "1474  One for the Money (Stephanie Plum Novels (Pape...     Janet Evanovich   \n",
       "1522                             Me Talk Pretty One Day       David Sedaris   \n",
       "1543                                    The Murder Book  Jonathan Kellerman   \n",
       "1578                                      The Rainmaker        John Grisham   \n",
       "\n",
       "     Year-Of-Publication                  Publisher  \\\n",
       "1456                1994  John Wiley &amp; Sons Inc   \n",
       "1474                1995                HarperTorch   \n",
       "1522                2001             Back Bay Books   \n",
       "1543                2003           Ballantine Books   \n",
       "1578                1995            Doubleday Books   \n",
       "\n",
       "                                            Image-URL-S  \\\n",
       "1456  http://images.amazon.com/images/P/002542730X.0...   \n",
       "1474  http://images.amazon.com/images/P/0061009059.0...   \n",
       "1522  http://images.amazon.com/images/P/0316776963.0...   \n",
       "1543  http://images.amazon.com/images/P/0345413903.0...   \n",
       "1578  http://images.amazon.com/images/P/0385424736.0...   \n",
       "\n",
       "                                            Image-URL-M  \\\n",
       "1456  http://images.amazon.com/images/P/002542730X.0...   \n",
       "1474  http://images.amazon.com/images/P/0061009059.0...   \n",
       "1522  http://images.amazon.com/images/P/0316776963.0...   \n",
       "1543  http://images.amazon.com/images/P/0345413903.0...   \n",
       "1578  http://images.amazon.com/images/P/0385424736.0...   \n",
       "\n",
       "                                            Image-URL-L  \n",
       "1456  http://images.amazon.com/images/P/002542730X.0...  \n",
       "1474  http://images.amazon.com/images/P/0061009059.0...  \n",
       "1522  http://images.amazon.com/images/P/0316776963.0...  \n",
       "1543  http://images.amazon.com/images/P/0345413903.0...  \n",
       "1578  http://images.amazon.com/images/P/0385424736.0...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#so 1266 users for 1000 books. Seems like a reasonable final dataset.\n",
    "#time to make final cuts on the catalog\n",
    "final_further_reduced_catalogCut = further_reduced_catalog[\"User-ID\"].isin(book_reviewers)\n",
    "final_further_reduced_catalog = further_reduced_catalog[final_further_reduced_catalogCut]\n",
    "final_further_reduced_catalog.head(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5179243971693922"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.rand(1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2. Setting up collaborative filtering\n",
    "Here, I build a collaborative filtering recommendation system for the book reviews that is based on an assignment from the Stanford Machine Learning Coursera. In that case, prepared film data from IMDB was used. Interestingly, I tracked down several bugs and flaws in the code developed for the assignment and fixed them in this implementation. Will note when applicable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1270)\n"
     ]
    }
   ],
   "source": [
    "#at this point, I am going to translate all the reviews in this catalog into a simple matrix containg reviews per user.\n",
    "#i am going to use a simple numpy matrix since the remainder of my code is prepared for such a use.\n",
    "# in the future, I will try to stick only  to Pandas framework\n",
    "#titles = np.array(most_reviewedbooks[\"Book-Title\"])\n",
    "Y = np.zeros((len(titles), len(book_reviewers)))\n",
    "print(Y.shape)\n",
    "num_books = Y.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#now we go through the final catalog line by line and fill in the elements of Y.\n",
    "#this is embarrasing and I'm sure there's a more efficient way to do this, but it's not too slow computationally for this dataset\n",
    "count = 0\n",
    "for row in final_further_reduced_catalog.values:\n",
    "    title_index = np.where(titles==row[3])[0][0]\n",
    "    user_index = np.where(book_reviewers==row[0])[0][0]   \n",
    "    Y[title_index][user_index]=row[2]\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert some additional user input. I personally went through the list of books and rated some.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I assigned this book:  The Da Vinci Code              \n",
      " This rating:  1\n",
      "I assigned this book:  The Nanny Diaries: A Novel        \n",
      " This rating:  1\n",
      "I assigned this book:  Harry Potter and the Chamber of Secrets (Book 2)              \n",
      " This rating:  5\n",
      "I assigned this book:  Harry Potter and the Sorcerer's Stone (Harry Potter (Paperback))           \n",
      " This rating:  5\n",
      "I assigned this book:  Angels &amp; Demons            \n",
      " This rating:  1\n",
      "I assigned this book:  Harry Potter and the Prisoner of Azkaban (Book 3)              \n",
      " This rating:  5\n",
      "I assigned this book:  Harry Potter and the Goblet of Fire (Book 4)               \n",
      " This rating:  5\n",
      "I assigned this book:  Harry Potter and the Order of the Phoenix (Book 5)                \n",
      " This rating:  5\n",
      "I assigned this book:  The Fellowship of the Ring (The Lord of the Rings, Part 1)              \n",
      " This rating:  8\n",
      "I assigned this book:  The Golden Compass (His Dark Materials, Book 1)                      \n",
      " This rating:  8\n",
      "I assigned this book:  American Gods                                     \n",
      " This rating:  8\n",
      "I assigned this book:  The Cider House Rules                            \n",
      " This rating:  8\n",
      "I assigned this book:  1984                                 \n",
      " This rating:  9\n",
      "I assigned this book:  The Handmaid's Tale                                 \n",
      " This rating:  7\n",
      "I assigned this book:  Animal Farm                                 \n",
      " This rating:  9\n",
      "I assigned this book:  The Subtle Knife (His Dark Materials, Book 2)                      \n",
      " This rating:  7\n",
      "I assigned this book:  The Little Prince                         \n",
      " This rating:  9\n",
      "I assigned this book:  Slaughterhouse Five or the Children's Crusade: A Duty Dance With Death                           \n",
      " This rating:  10\n",
      "I assigned this book:  Neuromancer (Remembering Tomorrow)                          \n",
      " This rating:  8\n",
      "I assigned this book:  Dune (Remembering Tomorrow)                             \n",
      " This rating:  8\n",
      "I assigned this book:  Atlas Shrugged                     \n",
      " This rating:  1\n",
      "I assigned this book:  Cat's Cradle                         \n",
      " This rating:  10\n",
      "I assigned this book:  The Stranger                            \n",
      " This rating:  9\n",
      "I assigned this book:  A Game of Thrones (A Song of Ice and Fire, Book 1)                   \n",
      " This rating:  8\n"
     ]
    }
   ],
   "source": [
    "#creating an extra column for the Y matrix \n",
    "my_ratings = np.zeros(num_books)\n",
    "#opening text file. This text file has the 1000 most reviewed books. One book per row.\n",
    "#For each book I rated, I assign a numerical score at the end of the line.\n",
    "ratings_file = open('sasha_top_books.txt')\n",
    "rating_lines = ratings_file.readlines()\n",
    "index_ratings_ar = []\n",
    "ratings_ar = []\n",
    "\n",
    "#I will use regular expression to tease out the books that have been rated.\n",
    "import re\n",
    "\n",
    "for line in rating_lines:\n",
    "    #look for all lines that end with a 1-2 digit number, and then a newline character\n",
    "    rating = re.findall('\\d+', line[-3:])\n",
    "    if (len(rating)>0):\n",
    "        rating_value = int(rating[0])\n",
    "        \n",
    "        #is the rating one or two characters?\n",
    "        if (rating_value>=10):\n",
    "            stripped_line = line[0:-3]\n",
    "        else:\n",
    "            stripped_line = line[0:-2]\n",
    "        \n",
    "        #now lets get rid of the rating and only keep the title\n",
    "        extra_stripped_line = stripped_line.strip()\n",
    "        \n",
    "        #make sure this title matches one in our list. also find the index. \n",
    "        rating_index = np.where(titles==extra_stripped_line)\n",
    "        if len(rating_index[0])>0:\n",
    "            index_ratings_ar.append(rating_index[0][0])\n",
    "            ratings_ar.append(rating_value)\n",
    "            print ('I assigned this book: ', stripped_line,'\\n This rating: ',rating_value)\n",
    "\n",
    "#now assigning the reviews to our column vector\n",
    "count = 0\n",
    "while (count<len(index_ratings_ar)):\n",
    "    my_ratings[index_ratings_ar[count]]=ratings_ar[count]\n",
    "    count+=1\n",
    "    \n",
    "Y = np.column_stack((my_ratings,Y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#the algorithm uses a second matrix R to tell if a given book has been reviewed by a given user\n",
    "R = Y>0\n",
    "num_users = Y.shape[1]\n",
    "num_books = Y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#now setting some parameters for the collaborative filtering\n",
    "#original Coursera version had 10 features, but I find better fits with 20. \n",
    "#lambda is the regularization parameter. 10 seems to work well\n",
    "#prefactor is an extra weight for the random seed. \n",
    "prefactor = 1\n",
    "my_lambda = 10.0\n",
    "num_features = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#do-penalties - we don't want the recommender to get fooled by books that have a high mean score\n",
    "#after it's been reviewed by only a single user. This is an element I added to the system to improve upon the quality of \n",
    "#recommendations given\n",
    "\n",
    "do_penalties = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#normalization function  that also returns mean score of each book and total number of reviews \n",
    "#for each book. \n",
    "#the algorithm works better when the scores are normalized (distance from mean)\n",
    "def normalizeY(YM):\n",
    "    cooper = np.copy(YM)\n",
    "    themeans = []\n",
    "    thecounts = []\n",
    "    for arr in cooper:\n",
    "        cut = arr>0\n",
    "        themean = np.mean(arr[cut])\n",
    "        count = len(arr[cut])\n",
    "        themeans.append(themean)\n",
    "        thecounts.append(float(count))\n",
    "        arr[cut] = arr[cut] - themean\n",
    "    themeans = np.array(themeans)\n",
    "    thecounts = np.array(thecounts)\n",
    "    return [cooper, themeans,thecounts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean count  28.65\n"
     ]
    }
   ],
   "source": [
    "#normalize the matrix, and find mean number of reviews per book\n",
    "#books that have fewer reviews than the mean will have their score weighted \n",
    "#so that it des not deviate too far form a \"neutral\" review \n",
    "\n",
    "[normY, themeans,thecounts] = normalizeY(Y)\n",
    "medcount = np.mean(thecounts)\n",
    "print('mean count ',medcount)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 1271\n",
      "(1000, 20)\n",
      "(1271, 20)\n"
     ]
    }
   ],
   "source": [
    "#now we initialize matrices for books and users. Each element contains a score per feature.\n",
    "#The features themselves are basically free variables that will be solved for by the optimizer\n",
    "num_users = normY.shape[1]\n",
    "print(num_books,num_users)\n",
    "X= np.random.randn(num_books, num_features)*prefactor\n",
    "Theta = np.random.randn(num_users, num_features)*prefactor\n",
    "\n",
    "print(X.shape)\n",
    "print(Theta.shape)\n",
    "\n",
    "#unroll both matrices into a single array to be used by the optimizer\n",
    "intial_parameters_unrolled  = np.append(np.array(X).reshape(-1), np.array(Theta).reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#now we define the cost function and the gradient of the cost function\n",
    "#fully vectorized implementation\n",
    "def cost_function_matrix_reshape_J(params, theY, theR, num_users, num_books, num_features, the_lambda):\n",
    "    J = 0\n",
    "    #unroll the vector back into two feture matrices\n",
    "    X = params[0:(num_books*num_features)]\n",
    "    X = X.reshape(num_books, num_features)\n",
    "    Theta = params[(num_books*num_features):]\n",
    "    Theta = Theta.reshape(num_users, num_features)\n",
    "\n",
    "    #multiply the two feature matrices, and see how close they are to the target Y.\n",
    "    #cost function is the difference of squares. Only considering books that have actual ratings in Y.\n",
    "    J_sub1 = np.multiply((np.dot(X,Theta.transpose()) - theY), theR)\n",
    "    J = np.sum(np.multiply(J_sub1,J_sub1)) / 2.0\n",
    "\n",
    "    #add in regularization\n",
    "    J +=  the_lambda*np.sum(np.multiply(X,X))/2.0  + the_lambda*np.sum(np.multiply(Theta,Theta))/2.0 \n",
    "    print('current cost is ',J)\n",
    "\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#fully vectorized implimentation of cost function gradient\n",
    "def cost_function_matrix_reshape_grad(params, theY, R, num_users, num_books, num_features, the_lambda):\n",
    "\n",
    "    #unroll the vector back into two feture matrices\n",
    "    X = params[0:(num_books*num_features)]\n",
    "    X = X.reshape(num_books, num_features)\n",
    "    Theta = params[(num_books*num_features):]\n",
    "    Theta = Theta.reshape(num_users, num_features)\n",
    "    \n",
    "    #similar calculation to computing J\n",
    "    Grad_sub1 = np.dot(X,Theta.transpose())\n",
    "    Grad_sub2 = np.multiply(Grad_sub1, R)\n",
    "    Grad_sub3 = Grad_sub2 - theY\n",
    "    \n",
    "    #but now there's an extra step to compute the gradient by using the other feature matrix\n",
    "    X_grad = np.dot(Grad_sub3, Theta)\n",
    "    Theta_grad = np.dot(Grad_sub3.transpose(), X )\n",
    "    \n",
    "    #regularization step\n",
    "    X_grad += the_lambda*X\n",
    "    Theta_grad += the_lambda*Theta\n",
    "    \n",
    "    #return the results\n",
    "    return np.append(X_grad.reshape(-1), Theta_grad.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current cost is  550104.165576\n",
      "current cost is  2.11967202187e+13\n",
      "current cost is  14657100362.0\n",
      "current cost is  6398377.61575\n",
      "current cost is  234650.546051\n",
      "current cost is  367625.523266\n",
      "current cost is  156801.188942\n",
      "current cost is  127556.870302\n",
      "current cost is  85416.6913585\n",
      "current cost is  90025.9441748\n",
      "current cost is  74446.8872318\n",
      "current cost is  60969.3961495\n",
      "current cost is  51804.9294014\n",
      "current cost is  48704.7069981\n",
      "current cost is  45927.8579308\n",
      "current cost is  39477.3241702\n",
      "current cost is  35242.5345732\n",
      "current cost is  38870.3538801\n",
      "current cost is  33540.9489378\n",
      "current cost is  32000.4723127\n",
      "current cost is  33106.957611\n",
      "current cost is  31263.0893538\n",
      "current cost is  31041.8213288\n",
      "current cost is  30822.9430517\n",
      "current cost is  30415.7190732\n",
      "current cost is  30321.7645528\n",
      "current cost is  30175.1658295\n",
      "current cost is  29878.0801533\n",
      "current cost is  30213.6258221\n",
      "current cost is  29780.123061\n",
      "current cost is  29650.4167899\n",
      "current cost is  29900.8358461\n",
      "current cost is  29617.0847029\n",
      "current cost is  29567.3276568\n",
      "current cost is  29552.0538661\n",
      "current cost is  29465.5270549\n",
      "current cost is  29368.9640882\n",
      "current cost is  29364.5976598\n",
      "current cost is  29318.8813095\n",
      "current cost is  29253.3438351\n",
      "current cost is  29277.6539639\n",
      "current cost is  29225.119454\n",
      "current cost is  29192.130941\n",
      "current cost is  29250.440033\n",
      "current cost is  29183.2755507\n",
      "current cost is  29173.0114809\n",
      "current cost is  29156.8213542\n",
      "current cost is  29149.3799667\n",
      "current cost is  29115.552154\n",
      "current cost is  29108.6562154\n",
      "current cost is  29066.6907177\n",
      "current cost is  29049.8123532\n",
      "current cost is  29040.1103837\n",
      "current cost is  29011.4626655\n",
      "current cost is  29039.8425436\n",
      "current cost is  29001.8365457\n",
      "current cost is  28988.6344776\n",
      "current cost is  29019.360644\n",
      "current cost is  28985.5037355\n",
      "current cost is  28981.0378525\n",
      "current cost is  28980.141176\n",
      "current cost is  28972.4535555\n",
      "current cost is  28970.8269782\n",
      "current cost is  28961.8307688\n",
      "current cost is  28950.3681863\n",
      "current cost is  28941.7202339\n",
      "current cost is  28945.159977\n",
      "current cost is  28937.9742106\n",
      "current cost is  28931.91689\n",
      "current cost is  28928.4749934\n",
      "current cost is  28921.0762992\n",
      "current cost is  28915.6731552\n",
      "current cost is  28911.1933947\n",
      "current cost is  28904.8443619\n",
      "current cost is  28903.6552383\n",
      "current cost is  28897.7234417\n",
      "current cost is  28902.8462875\n",
      "current cost is  28895.6282472\n",
      "current cost is  28892.5044234\n",
      "current cost is  28891.5952884\n",
      "current cost is  28890.6337508\n",
      "current cost is  28889.2772892\n",
      "current cost is  28886.6581689\n",
      "current cost is  28884.4637281\n",
      "current cost is  28883.3044823\n",
      "current cost is  28882.96566\n",
      "current cost is  28880.3518091\n",
      "current cost is  28877.4187497\n",
      "current cost is  28878.4336211\n",
      "current cost is  28874.8363744\n",
      "current cost is  28871.2379876\n",
      "current cost is  28868.6064301\n",
      "current cost is  28878.0490729\n",
      "current cost is  28868.1294394\n",
      "current cost is  28867.2588944\n",
      "current cost is  28865.6494382\n",
      "current cost is  28865.0391999\n",
      "current cost is  28864.2173314\n",
      "current cost is  28862.2653013\n",
      "current cost is  28861.1003934\n",
      "current cost is  28860.867277\n",
      "current cost is  28859.3649979\n",
      "current cost is  28857.4516697\n",
      "current cost is  28855.9049546\n",
      "current cost is  28858.0008847\n",
      "current cost is  28855.4335433\n",
      "current cost is  28854.6245454\n",
      "current cost is  28853.8503388\n",
      "current cost is  28852.4098685\n",
      "current cost is  28851.3146108\n",
      "current cost is  28850.078635\n",
      "current cost is  28848.5274327\n",
      "current cost is  28852.7681088\n",
      "current cost is  28848.1938218\n",
      "current cost is  28847.7801465\n",
      "current cost is  28847.4133055\n",
      "current cost is  28846.9418185\n",
      "current cost is  28846.4751662\n",
      "current cost is  28846.1055943\n",
      "current cost is  28845.5917064\n",
      "current cost is  28845.0136714\n",
      "current cost is  28844.2833112\n",
      "current cost is  28843.2587126\n",
      "current cost is  28842.8409411\n",
      "current cost is  28842.6099437\n",
      "current cost is  28841.7323538\n",
      "current cost is  28840.5686423\n",
      "current cost is  28839.8937264\n",
      "current cost is  28839.7460548\n",
      "current cost is  28839.2806379\n",
      "current cost is  28839.1692663\n",
      "current cost is  28838.1692145\n",
      "current cost is  28837.1289016\n",
      "current cost is  28835.3596984\n",
      "current cost is  28832.4361324\n",
      "current cost is  28831.2427379\n",
      "current cost is  28831.6133019\n",
      "current cost is  28829.2559014\n",
      "current cost is  28828.5395091\n",
      "current cost is  28828.0369368\n",
      "current cost is  28826.8024929\n",
      "current cost is  28826.9284792\n",
      "current cost is  28826.2097426\n",
      "current cost is  28825.5179943\n",
      "current cost is  28824.7324075\n",
      "current cost is  28824.7189621\n",
      "current cost is  28824.3319517\n",
      "current cost is  28823.9613902\n",
      "current cost is  28823.3420376\n",
      "current cost is  28822.8761526\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 28822.876153\n",
      "         Iterations: 100\n",
      "         Function evaluations: 150\n",
      "         Gradient evaluations: 150\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import fmin_cg\n",
    "\n",
    "#now we run the optimizer fmin_cg to minimize the cost function.\n",
    "\n",
    "results = fmin_cg(cost_function_matrix_reshape_J,intial_parameters_unrolled,fprime=cost_function_matrix_reshape_grad,\\\n",
    "                      args=(normY,R,num_users,num_books,num_features,my_lambda), maxiter=100,disp=True,full_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking at the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " my newX (1000, 20)\n",
      "my newTheta  (1271, 20)\n"
     ]
    }
   ],
   "source": [
    "#unroll the results vector back into optimized feature matrices\n",
    "returns = results[0]\n",
    "newX = returns[:(num_books*num_features)]\n",
    "newX = newX.reshape(num_books, num_features)\n",
    "print(' my newX', newX.shape)\n",
    "newTheta = returns[(num_books*num_features):]\n",
    "newTheta = newTheta.reshape(num_users, num_features)\n",
    "print( 'my newTheta ',newTheta.shape)\n",
    "p = np.dot(newX,  newTheta.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#add the means back in\n",
    "my_predictions = p[:,0] + themeans\n",
    "#this is where i penalize scores of books that have fewer than the mean number of reviews\n",
    "cut = (thecounts<medcount)\n",
    "\n",
    "# i set it up such that if the book has no reviews, it is assigned a score of 5\n",
    "# if the book has more than the mean number of reviews, it is assigned whatever actual scores it got.\n",
    "# if a book has in between zero and n_mean reviews, it is given a linear combination of  5 and the predicted\n",
    "# score based on the distance to each\n",
    "if (do_penalties):\n",
    "    my_predictions[cut] = ((medcount-thecounts[cut])/medcount) * 5.0 + (thecounts[cut]/medcount)*my_predictions[cut]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 books to read\n",
      "Book to read:  A Prayer for Owen Meany \n",
      " Predicted score:  10.2668871431\n",
      "Book to read:  The Two Towers (The Lord of the Rings, Part 2) \n",
      " Predicted score:  9.64657410708\n",
      "Book to read:  The Return of the King (The Lord of the Rings, Part 3) \n",
      " Predicted score:  9.64035566851\n",
      "Book to read:  Anne of Green Gables (Anne of Green Gables Novels (Paperback)) \n",
      " Predicted score:  9.19946526176\n",
      "Book to read:  The Amber Spyglass (His Dark Materials, Book 3) \n",
      " Predicted score:  9.17322941454\n",
      "Book to read:  The Murder Book \n",
      " Predicted score:  9.15372221774\n",
      "Book to read:  The Green Mile \n",
      " Predicted score:  9.11527703118\n",
      "Book to read:  Griffin &amp; Sabine: An Extraordinary Correspondence \n",
      " Predicted score:  8.95769808545\n",
      "Book to read:  The Little Prince \n",
      " Predicted score:  8.95516435321\n",
      "Book to read:  The Secret Garden \n",
      " Predicted score:  8.93417682364\n",
      "Book to read:  To Kill a Mockingbird \n",
      " Predicted score:  8.81701065006\n",
      "Book to read:  The Blue Nowhere : A Novel \n",
      " Predicted score:  8.78558041275\n",
      "Book to read:  Slaughterhouse Five or the Children's Crusade: A Duty Dance With Death \n",
      " Predicted score:  8.76038015995\n",
      "Book to read:  Gone with the Wind \n",
      " Predicted score:  8.72977930871\n",
      "Book to read:  Holes (Yearling Newbery) \n",
      " Predicted score:  8.72504169775\n",
      "Book to read:  Big Stone Gap: A Novel (Ballantine Reader's Circle) \n",
      " Predicted score:  8.67920574603\n",
      "Book to read:  Wuthering Heights \n",
      " Predicted score:  8.63617818721\n",
      "Book to read:  Lies and the Lying Liars Who Tell Them: A Fair and Balanced Look at the Right \n",
      " Predicted score:  8.63535273474\n",
      "Book to read:  Seabiscuit: An American Legend \n",
      " Predicted score:  8.61450953408\n",
      "Book to read:  One for the Money (A Stephanie Plum Novel) \n",
      " Predicted score:  8.60681884243\n"
     ]
    }
   ],
   "source": [
    "#print top 20 best rated book.\n",
    "fingers = np.argsort(-1*my_predictions)\n",
    "print(\"Top 20 books to read\")\n",
    "\n",
    "for i in range(0, 20):\n",
    "    print ('Book to read: ',titles[fingers][i], '\\n Predicted score: ', my_predictions[fingers][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As a bonus, here are some books you shouldn't bother reading\n",
      "Book not to read:  The Da Vinci Code \n",
      " Predicted score:  2.79580700553\n",
      "Book not to read:  The Nanny Diaries: A Novel \n",
      " Predicted score:  2.79873485697\n",
      "Book not to read:  Angels &amp; Demons \n",
      " Predicted score:  3.45142597152\n",
      "Book not to read:  The Bourne Identity \n",
      " Predicted score:  4.39581883907\n",
      "Book not to read:  Four Blondes \n",
      " Predicted score:  4.41975273816\n",
      "Book not to read:  Wild Animus \n",
      " Predicted score:  4.71834373755\n",
      "Book not to read:  Deception Point \n",
      " Predicted score:  4.81216529989\n",
      "Book not to read:  Dreamcatcher \n",
      " Predicted score:  4.89996012911\n",
      "Book not to read:  The Wedding \n",
      " Predicted score:  5.05152966422\n",
      "Book not to read:  Russendisko. \n",
      " Predicted score:  5.06980945474\n",
      "Book not to read:  Standoff \n",
      " Predicted score:  5.10154975541\n",
      "Book not to read:  Free \n",
      " Predicted score:  5.10471489511\n",
      "Book not to read:  The Celestine Prophecy \n",
      " Predicted score:  5.1047551718\n",
      "Book not to read:  The Little Friend \n",
      " Predicted score:  5.10873359156\n",
      "Book not to read:  The Shelters of Stone (Earth's Children Series, No 5) \n",
      " Predicted score:  5.16770206126\n",
      "Book not to read:  The Tommyknockers \n",
      " Predicted score:  5.21142611313\n",
      "Book not to read:  The Corrections: A Novel \n",
      " Predicted score:  5.2297055152\n",
      "Book not to read:  Stupid White Men. Eine Abrechnung mit dem Amerika unter George W. Bush \n",
      " Predicted score:  5.24667700919\n",
      "Book not to read:  The Pilot's Wife : A Novel \n",
      " Predicted score:  5.25664466092\n",
      "Book not to read:  Pagan Babies \n",
      " Predicted score:  5.25795571671\n"
     ]
    }
   ],
   "source": [
    "fingers = np.argsort(my_predictions)\n",
    "print(\"As a bonus, here are some books you shouldn't bother reading\")\n",
    "for i in range(0, 20):\n",
    "    print ('Book not to read: ',titles[fingers][i], '\\n Predicted score: ', my_predictions[fingers][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Overall, I am pleased that this algorithm works. It (correctly) recommends books that I have already read and rated highly. If I really only wanted new books to read, I would obviously filter those out, but I kept them in as a sanity check. It recommends more Lord of the Rings book after I said I liked the first one. It also recommends the final His Dark Materials book, after I rated the first two pretty highly.\n",
    "\n",
    "On the other hand, the level of customization is not exactly top-notch.  I have tried it with various inputs, and it tends to always recommend a few specific titles. A Prayer for Owen Meany, Griffin & Sabine, Anne of Green Gables, and The Secret Garden are favorites. I suspect this sort of collaborative filtering is prone to function in this way. Most people only finish reading something that they are already invested in, and would rate highly. I think it's different than the small amount of investment that people would put in for a movie review. For a second pass at this problem, I would probably try association algorithms using decision trees instead of collaborative filtering. \n",
    "\n",
    "Nonetheless, there are probably a few solid recommendations in this list. I am actually pretty intrigued by Griffin & Sabine. This was a fun project! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "   # 4. Evaluating the effectiveness of the project!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Lets put some more work into this now! I think we can do some interesting things with the data. My first thought on how this should be done is to withhold about 10% of the data from the algorithm, and see how effectively it captures it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1270)\n"
     ]
    }
   ],
   "source": [
    "trainY = np.zeros((len(titles), len(book_reviewers)))\n",
    "print(trainY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "number_of_entries = len(final_further_reduced_catalog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_or_test = np.random.rand(number_of_entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "\n",
    "test_title_indeces = []\n",
    "test_user_indeces = []\n",
    "test_scores = []\n",
    "\n",
    "for row in final_further_reduced_catalog.values:\n",
    "    title_index = np.where(titles==row[3])[0][0]\n",
    "    user_index = np.where(book_reviewers==row[0])[0][0]   \n",
    "    if (train_or_test[count]< 0.9):\n",
    "        trainY[title_index][user_index]=row[2]\n",
    "    else:\n",
    "        test_title_indeces.append(title_index)\n",
    "        test_user_indeces.append(user_index)\n",
    "        test_scores.append(row[2])\n",
    "    count+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2945"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_test_scores = len(test_scores)\n",
    "\n",
    "len(test_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainR = trainY>0\n",
    "train_num_users = trainY.shape[1]\n",
    "train_num_books = trainY.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean count  25.777\n"
     ]
    }
   ],
   "source": [
    "[train_normY, train_themeans,train_thecounts] = normalizeY(trainY)\n",
    "train_medcount = np.mean(train_thecounts)\n",
    "print('mean count ',train_medcount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 20)\n",
      "(1270, 20)\n"
     ]
    }
   ],
   "source": [
    "trainX= np.random.randn(train_num_books, num_features)*prefactor\n",
    "trainTheta = np.random.randn(train_num_users, num_features)*prefactor\n",
    "\n",
    "print(trainX.shape)\n",
    "print(trainTheta.shape)\n",
    "\n",
    "#unroll both matrices into a single array to be used by the optimizer\n",
    "intial_parameters_unrolled  = np.append(np.array(trainX).reshape(-1), np.array(trainTheta).reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current cost is  517845.610632\n",
      "current cost is  1.8298925724e+13\n",
      "current cost is  12615337778.5\n",
      "current cost is  5434611.68128\n",
      "current cost is  242835.974242\n",
      "current cost is  399768.014472\n",
      "current cost is  153449.978698\n",
      "current cost is  233331.600503\n",
      "current cost is  135452.820487\n",
      "current cost is  107236.332169\n",
      "current cost is  65065.2064794\n",
      "current cost is  284869.071536\n",
      "current cost is  56264.9016931\n",
      "current cost is  43283.7159591\n",
      "current cost is  38999.1009709\n",
      "current cost is  35659.1026829\n",
      "current cost is  35242.7961622\n",
      "current cost is  30934.1715035\n",
      "current cost is  38274.6024514\n",
      "current cost is  29560.5720136\n",
      "current cost is  29723.63215\n",
      "current cost is  28809.8962729\n",
      "current cost is  28326.8144957\n",
      "current cost is  29016.9598747\n",
      "current cost is  28167.3553123\n",
      "current cost is  27909.3674372\n",
      "current cost is  27748.6770211\n",
      "current cost is  28053.6898017\n",
      "current cost is  27594.7864913\n",
      "current cost is  27419.0170387\n",
      "current cost is  27241.8779062\n",
      "current cost is  27153.9612979\n",
      "current cost is  27119.4839674\n",
      "current cost is  27189.9422883\n",
      "current cost is  27069.5356968\n",
      "current cost is  26998.9245132\n",
      "current cost is  27086.0077886\n",
      "current cost is  26977.1408045\n",
      "current cost is  26949.0482354\n",
      "current cost is  26918.4573785\n",
      "current cost is  26911.6473891\n",
      "current cost is  26901.1629019\n",
      "current cost is  26871.1752355\n",
      "current cost is  26839.8493726\n",
      "current cost is  27007.8447605\n",
      "current cost is  26826.7362317\n",
      "current cost is  26805.5694745\n",
      "current cost is  26793.847981\n",
      "current cost is  26745.9660339\n",
      "current cost is  26735.6770407\n",
      "current cost is  26945.1155884\n",
      "current cost is  26724.9132775\n",
      "current cost is  26710.4422804\n",
      "current cost is  26702.5965279\n",
      "current cost is  26700.5274173\n",
      "current cost is  26693.5794002\n",
      "current cost is  26688.2570988\n",
      "current cost is  26682.5103282\n",
      "current cost is  26674.2108965\n",
      "current cost is  26672.3953787\n",
      "current cost is  26668.600851\n",
      "current cost is  26666.0915782\n",
      "current cost is  26663.8938073\n",
      "current cost is  26662.2407557\n",
      "current cost is  26656.877653\n",
      "current cost is  26648.7769399\n",
      "current cost is  26646.1404367\n",
      "current cost is  26641.3641713\n",
      "current cost is  26639.1916202\n",
      "current cost is  26628.8385587\n",
      "current cost is  26625.6309546\n",
      "current cost is  26631.6361836\n",
      "current cost is  26620.0027043\n",
      "current cost is  26610.8360749\n",
      "current cost is  26605.6322137\n",
      "current cost is  26613.3136192\n",
      "current cost is  26599.8763201\n",
      "current cost is  26596.4862867\n",
      "current cost is  26595.774001\n",
      "current cost is  26590.531723\n",
      "current cost is  26597.6292091\n",
      "current cost is  26588.9465673\n",
      "current cost is  26586.953881\n",
      "current cost is  26588.3250549\n",
      "current cost is  26586.2052507\n",
      "current cost is  26585.4252648\n",
      "current cost is  26584.0174867\n",
      "current cost is  26581.7278063\n",
      "current cost is  26576.0828262\n",
      "current cost is  26574.1059978\n",
      "current cost is  26578.7902723\n",
      "current cost is  26571.1341802\n",
      "current cost is  26567.6896837\n",
      "current cost is  26564.6551014\n",
      "current cost is  26563.7893194\n",
      "current cost is  26562.8704441\n",
      "current cost is  26560.6739817\n",
      "current cost is  26558.8235687\n",
      "current cost is  26559.0039201\n",
      "current cost is  26557.9320263\n",
      "current cost is  26556.6857008\n",
      "current cost is  26559.1091884\n",
      "current cost is  26556.3650809\n",
      "current cost is  26555.8809855\n",
      "current cost is  26555.7239591\n",
      "current cost is  26554.7424734\n",
      "current cost is  26554.3839998\n",
      "current cost is  26552.2480782\n",
      "current cost is  26551.2079191\n",
      "current cost is  26553.5174985\n",
      "current cost is  26550.0221014\n",
      "current cost is  26548.9753248\n",
      "current cost is  26548.1498384\n",
      "current cost is  26546.9452218\n",
      "current cost is  26546.6551969\n",
      "current cost is  26545.9211943\n",
      "current cost is  26545.6574983\n",
      "current cost is  26544.1354836\n",
      "current cost is  26543.6054283\n",
      "current cost is  26542.3058383\n",
      "current cost is  26542.62043\n",
      "current cost is  26541.7103947\n",
      "current cost is  26540.7028743\n",
      "current cost is  26539.8584237\n",
      "current cost is  26539.3618253\n",
      "current cost is  26538.7794508\n",
      "current cost is  26537.9447524\n",
      "current cost is  26537.4604919\n",
      "current cost is  26537.3536122\n",
      "current cost is  26536.6936113\n",
      "current cost is  26536.678768\n",
      "current cost is  26536.3566605\n",
      "current cost is  26535.9824921\n",
      "current cost is  26535.7083255\n",
      "current cost is  26535.4624555\n",
      "current cost is  26535.0447264\n",
      "current cost is  26534.6815544\n",
      "current cost is  26534.3487955\n",
      "current cost is  26534.1814492\n",
      "current cost is  26533.7112536\n",
      "current cost is  26533.2190518\n",
      "current cost is  26532.7476827\n",
      "current cost is  26532.4371857\n",
      "current cost is  26532.1648589\n",
      "current cost is  26531.7792919\n",
      "current cost is  26531.705093\n",
      "current cost is  26531.6732402\n",
      "current cost is  26531.4646256\n",
      "current cost is  26531.1227344\n",
      "current cost is  26531.054664\n",
      "current cost is  26530.5288454\n",
      "current cost is  26530.2656612\n",
      "current cost is  26530.1759224\n",
      "current cost is  26530.3112696\n",
      "current cost is  26530.0261852\n",
      "current cost is  26529.7983293\n",
      "current cost is  26529.7196406\n",
      "current cost is  26529.3116295\n",
      "current cost is  26529.2296375\n",
      "current cost is  26529.0827073\n",
      "current cost is  26529.2351447\n",
      "current cost is  26528.9957061\n",
      "current cost is  26528.8660795\n",
      "current cost is  26528.8283329\n",
      "current cost is  26528.5970033\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 26528.597003\n",
      "         Iterations: 100\n",
      "         Function evaluations: 165\n",
      "         Gradient evaluations: 165\n"
     ]
    }
   ],
   "source": [
    "train_results = fmin_cg(cost_function_matrix_reshape_J,intial_parameters_unrolled,fprime=cost_function_matrix_reshape_grad,\\\n",
    "                      args=(train_normY,trainR,train_num_users,train_num_books,num_features,my_lambda), maxiter=100,disp=True,full_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " my train_newX (1000, 20)\n",
      "my train_newTheta  (1270, 20)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1000, 1270)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_returns = train_results[0]\n",
    "train_newX = train_returns[:(train_num_books*num_features)]\n",
    "train_newX = train_newX.reshape(train_num_books, num_features)\n",
    "print(' my train_newX', train_newX.shape)\n",
    "train_newTheta = train_returns[(train_num_books*num_features):]\n",
    "train_newTheta = train_newTheta.reshape(train_num_users, num_features)\n",
    "print( 'my train_newTheta ',train_newTheta.shape)\n",
    "train_p = np.dot(train_newX,  train_newTheta.transpose())\n",
    "train_p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#how accurate was the matrix that was trained with 90% of the data at guessing the remaining 10%? \n",
    "good_count = 0\n",
    "bad_count = 0\n",
    "for i in range(0,num_test_scores):\n",
    "    #trained_score = train_p[test_title_indeces[i],test_user_indeces[i]] + themeans[test_title_indeces[i]]\n",
    "    trained_adjustment = train_p[test_title_indeces[i],test_user_indeces[i]]\n",
    "    mean_for_book =  train_themeans[test_title_indeces[i]]\n",
    "    trained_score = trained_adjustment+mean_for_book\n",
    "    if (abs(trained_adjustment) > 1): \n",
    "        #print(\"entry \", i,\": the real score was \", test_scores[i],\" trained score \",trained_score)\n",
    "        #print(\"mean was \", mean_for_book,\" adjustment \",trained_adjustment)\n",
    "        distance = abs( test_scores[i] - trained_score)\n",
    "        dist_to_mean = abs(test_scores[i] -mean_for_book )\n",
    "        if (distance < dist_to_mean):\n",
    "            good_count+=1\n",
    "        else:\n",
    "            bad_count+=1\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "143"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fraction_of_good_adjustments = (good_count)/(good_count+bad_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7333333333333333"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fraction_of_good_adjustments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#using matrix that was actually trained with 100% of the data. \n",
    "good_count = 0\n",
    "bad_count = 0\n",
    "for i in range(0,num_test_scores):\n",
    "    #trained_score = train_p[test_title_indeces[i],test_user_indeces[i]] + themeans[test_title_indeces[i]]\n",
    "    trained_adjustment = p[test_title_indeces[i],test_user_indeces[i]+1]\n",
    "    mean_for_book =  train_themeans[test_title_indeces[i]]\n",
    "    trained_score = trained_adjustment+mean_for_book\n",
    "    if (abs(trained_adjustment) > 1): \n",
    "        #print(\"entry \", i,\": the real score was \", test_scores[i],\" trained score \",trained_score)\n",
    "        #print(\"mean was \", mean_for_book,\" adjustment \",trained_adjustment)\n",
    "        distance = abs( test_scores[i] - trained_score)\n",
    "        dist_to_mean = abs(test_scores[i] -mean_for_book )\n",
    "        if (distance < dist_to_mean):\n",
    "            good_count+=1\n",
    "        else:\n",
    "            bad_count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fraction_of_good_adjustments = (good_count)/(good_count+bad_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9819639278557114"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fraction_of_good_adjustments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
