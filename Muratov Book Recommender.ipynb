{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alexander Muratov's Book Recommdner\n",
    "\n",
    "I have often wondered why Goodreads, Amazon, and Audible just don't really do a good job in figuring out which books to recommend to me. Perhaps books are a little too personal and situational compared to movies and music. You invest much more time, and often can't boil down your review to just a number. How can you put a work of classic literature alongside a fun comic book? \n",
    "\n",
    "But I decided that I would give it a shot anyway. I decided to try the collaborative filtering algorithm I learned in Andrew Ng's coursera Machine Learning course on a public book rating dataset, and see what I get. Perhaps it would give me some cool ideas for what I should be reading, or at least help me see the challenges in solving this problem.\n",
    "\n",
    "\n",
    "## Using data from  Book-Crossing Dataset \n",
    "http://www2.informatik.uni-freiburg.de/~cziegler/BX/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1. Data Cleaning \n",
    "Here, I am going to take the original raw data files and pull out only the information I need to build my recommender system.\n",
    "The code presented here is a significantly streamlined version of the first time I actually tried to explore this data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#first CSV file gives ratings for users for books by code number, but no titles\n",
    "data = pd.read_csv(\"BookData_A/BX-Book-Ratings.csv\", encoding='latin-1', sep=';')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 6452: expected 8 fields, saw 9\\nSkipping line 43667: expected 8 fields, saw 10\\nSkipping line 51751: expected 8 fields, saw 9\\n'\n",
      "b'Skipping line 92038: expected 8 fields, saw 9\\nSkipping line 104319: expected 8 fields, saw 9\\nSkipping line 121768: expected 8 fields, saw 9\\n'\n",
      "b'Skipping line 144058: expected 8 fields, saw 9\\nSkipping line 150789: expected 8 fields, saw 9\\nSkipping line 157128: expected 8 fields, saw 9\\nSkipping line 180189: expected 8 fields, saw 9\\nSkipping line 185738: expected 8 fields, saw 9\\n'\n",
      "b'Skipping line 209388: expected 8 fields, saw 9\\nSkipping line 220626: expected 8 fields, saw 9\\nSkipping line 227933: expected 8 fields, saw 11\\nSkipping line 228957: expected 8 fields, saw 10\\nSkipping line 245933: expected 8 fields, saw 9\\nSkipping line 251296: expected 8 fields, saw 9\\nSkipping line 259941: expected 8 fields, saw 9\\nSkipping line 261529: expected 8 fields, saw 9\\n'\n",
      "C:\\Users\\Alejandro\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2723: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "#second CSV file gives book titles. Some of the titles have weird characters, so it must be read with latin-1 encoding\n",
    "#This was a painful thing to figure out. \n",
    "bookdata = pd.read_csv(\"BookData_A/BX-Books.csv\", encoding='latin-1', sep=';',error_bad_lines=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User-ID</th>\n",
       "      <th>ISBN</th>\n",
       "      <th>Book-Rating</th>\n",
       "      <th>Book-Title</th>\n",
       "      <th>Book-Author</th>\n",
       "      <th>Year-Of-Publication</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>Image-URL-S</th>\n",
       "      <th>Image-URL-M</th>\n",
       "      <th>Image-URL-L</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>276726</td>\n",
       "      <td>0155061224</td>\n",
       "      <td>5</td>\n",
       "      <td>Rites of Passage</td>\n",
       "      <td>Judith Rae</td>\n",
       "      <td>2001</td>\n",
       "      <td>Heinle</td>\n",
       "      <td>http://images.amazon.com/images/P/0155061224.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0155061224.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0155061224.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>276729</td>\n",
       "      <td>052165615X</td>\n",
       "      <td>3</td>\n",
       "      <td>Help!: Level 1</td>\n",
       "      <td>Philip Prowse</td>\n",
       "      <td>1999</td>\n",
       "      <td>Cambridge University Press</td>\n",
       "      <td>http://images.amazon.com/images/P/052165615X.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/052165615X.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/052165615X.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>276729</td>\n",
       "      <td>0521795028</td>\n",
       "      <td>6</td>\n",
       "      <td>The Amsterdam Connection : Level 4 (Cambridge ...</td>\n",
       "      <td>Sue Leather</td>\n",
       "      <td>2001</td>\n",
       "      <td>Cambridge University Press</td>\n",
       "      <td>http://images.amazon.com/images/P/0521795028.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0521795028.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0521795028.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>276744</td>\n",
       "      <td>038550120X</td>\n",
       "      <td>7</td>\n",
       "      <td>A Painted House</td>\n",
       "      <td>JOHN GRISHAM</td>\n",
       "      <td>2001</td>\n",
       "      <td>Doubleday</td>\n",
       "      <td>http://images.amazon.com/images/P/038550120X.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/038550120X.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/038550120X.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>276747</td>\n",
       "      <td>0060517794</td>\n",
       "      <td>9</td>\n",
       "      <td>Little Altars Everywhere</td>\n",
       "      <td>Rebecca Wells</td>\n",
       "      <td>2003</td>\n",
       "      <td>HarperTorch</td>\n",
       "      <td>http://images.amazon.com/images/P/0060517794.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0060517794.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0060517794.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    User-ID        ISBN  Book-Rating  \\\n",
       "1    276726  0155061224            5   \n",
       "3    276729  052165615X            3   \n",
       "4    276729  0521795028            6   \n",
       "8    276744  038550120X            7   \n",
       "16   276747  0060517794            9   \n",
       "\n",
       "                                           Book-Title    Book-Author  \\\n",
       "1                                    Rites of Passage     Judith Rae   \n",
       "3                                      Help!: Level 1  Philip Prowse   \n",
       "4   The Amsterdam Connection : Level 4 (Cambridge ...    Sue Leather   \n",
       "8                                     A Painted House   JOHN GRISHAM   \n",
       "16                           Little Altars Everywhere  Rebecca Wells   \n",
       "\n",
       "   Year-Of-Publication                   Publisher  \\\n",
       "1                 2001                      Heinle   \n",
       "3                 1999  Cambridge University Press   \n",
       "4                 2001  Cambridge University Press   \n",
       "8                 2001                   Doubleday   \n",
       "16                2003                 HarperTorch   \n",
       "\n",
       "                                          Image-URL-S  \\\n",
       "1   http://images.amazon.com/images/P/0155061224.0...   \n",
       "3   http://images.amazon.com/images/P/052165615X.0...   \n",
       "4   http://images.amazon.com/images/P/0521795028.0...   \n",
       "8   http://images.amazon.com/images/P/038550120X.0...   \n",
       "16  http://images.amazon.com/images/P/0060517794.0...   \n",
       "\n",
       "                                          Image-URL-M  \\\n",
       "1   http://images.amazon.com/images/P/0155061224.0...   \n",
       "3   http://images.amazon.com/images/P/052165615X.0...   \n",
       "4   http://images.amazon.com/images/P/0521795028.0...   \n",
       "8   http://images.amazon.com/images/P/038550120X.0...   \n",
       "16  http://images.amazon.com/images/P/0060517794.0...   \n",
       "\n",
       "                                          Image-URL-L  \n",
       "1   http://images.amazon.com/images/P/0155061224.0...  \n",
       "3   http://images.amazon.com/images/P/052165615X.0...  \n",
       "4   http://images.amazon.com/images/P/0521795028.0...  \n",
       "8   http://images.amazon.com/images/P/038550120X.0...  \n",
       "16  http://images.amazon.com/images/P/0060517794.0...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Merging the two CSV files to get a catalog with book titles and ratings \n",
    "catalog = data.merge(bookdata, on='ISBN', how='left')\n",
    "\n",
    "#Getting rid of non-reviews - entries with a rating of 0\n",
    "#also getting rid of book titles that are NULL value.\n",
    "reduced_catalog = catalog[(catalog['Book-Rating']>0) & (catalog['Book-Title'].notnull())]\n",
    "\n",
    "\n",
    "#check to make sure catalog looks ok\n",
    "reduced_catalog.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['The Lovely Bones: A Novel', 'Wild Animus', 'The Da Vinci Code',\n",
       "       'The Secret Life of Bees', 'The Nanny Diaries: A Novel'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mycounts = reduced_catalog[\"Book-Title\"].value_counts()\n",
    "mycounts.index[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['The Lovely Bones: A Novel', 'Wild Animus', 'The Da Vinci Code',\n",
       "       'The Secret Life of Bees', 'The Nanny Diaries: A Novel'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#looks good. Now lets figure out which books are the ones that are reviewed most often to feed to the recommender system \n",
    "\n",
    "#lets start with a catalog of 1000 most reviewed books\n",
    "num_top_books = 1000\n",
    "\n",
    "countsorted_reviewed_books =  reduced_catalog[\"Book-Title\"].value_counts()\n",
    "most_reviewedbooks = countsorted_reviewed_books.index[0:num_top_books]\n",
    "most_reviewedbooks[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Personal Injuries',\n",
       "       'Dying for Chocolate (Culinary Mysteries (Paperback))', 'Guilty as Sin',\n",
       "       'Love You Forever'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets also make sure that the ones at the bottom of the list have enough reviews\n",
    "most_reviewedbooks[-5:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#not bad. Seems like it will be a robust list\n",
    "#because I am still not totally used to working with dataframes and my algorithm is designed for simple matrices and arrays\n",
    "#I am going to start migrating some stuff to arrays now\n",
    "titles = np.array( most_reviewedbooks)\n",
    "\n",
    "#if someone wants to review the books themselves, uncomment to save text file\n",
    "#np.savetxt(\"top_book_titles.txt\",titles, fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#reducing the dataframe to only contain data for top X books\n",
    "\n",
    "further_reduced_catalogCut = reduced_catalog[\"Book-Title\"].isin(titles)\n",
    "further_reduced_catalog = reduced_catalog[further_reduced_catalogCut]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1266,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now lets get a set of the most active reviewers \n",
    "\n",
    "reviewingusers =  further_reduced_catalog[\"User-ID\"].value_counts()\n",
    "\n",
    "#i don't want any reviewers with fewer than 10 reviews.\n",
    "top_users = reviewingusers[reviewingusers.values>10]\n",
    "book_reviewers = np.array(top_users.index)\n",
    "book_reviewers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#I'm going to subddivide users into a test set and training set.\n",
    "#as of now I am not actually doing this step, but if I wanted  to validate the recommender system,\n",
    "#I would use only 70% of the data t otrain the model, and the other 30% to test. \n",
    "use_set = np.random.rand(top_users.shape[0]) > 0.7 \n",
    "test_set = np.invert(use_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User-ID</th>\n",
       "      <th>ISBN</th>\n",
       "      <th>Book-Rating</th>\n",
       "      <th>Book-Title</th>\n",
       "      <th>Book-Author</th>\n",
       "      <th>Year-Of-Publication</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>Image-URL-S</th>\n",
       "      <th>Image-URL-M</th>\n",
       "      <th>Image-URL-L</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>277427</td>\n",
       "      <td>002542730X</td>\n",
       "      <td>10</td>\n",
       "      <td>Politically Correct Bedtime Stories: Modern Ta...</td>\n",
       "      <td>James Finn Garner</td>\n",
       "      <td>1994</td>\n",
       "      <td>John Wiley &amp;amp; Sons Inc</td>\n",
       "      <td>http://images.amazon.com/images/P/002542730X.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/002542730X.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/002542730X.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1474</th>\n",
       "      <td>277427</td>\n",
       "      <td>0061009059</td>\n",
       "      <td>9</td>\n",
       "      <td>One for the Money (Stephanie Plum Novels (Pape...</td>\n",
       "      <td>Janet Evanovich</td>\n",
       "      <td>1995</td>\n",
       "      <td>HarperTorch</td>\n",
       "      <td>http://images.amazon.com/images/P/0061009059.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0061009059.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0061009059.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1522</th>\n",
       "      <td>277427</td>\n",
       "      <td>0316776963</td>\n",
       "      <td>8</td>\n",
       "      <td>Me Talk Pretty One Day</td>\n",
       "      <td>David Sedaris</td>\n",
       "      <td>2001</td>\n",
       "      <td>Back Bay Books</td>\n",
       "      <td>http://images.amazon.com/images/P/0316776963.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0316776963.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0316776963.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1543</th>\n",
       "      <td>277427</td>\n",
       "      <td>0345413903</td>\n",
       "      <td>10</td>\n",
       "      <td>The Murder Book</td>\n",
       "      <td>Jonathan Kellerman</td>\n",
       "      <td>2003</td>\n",
       "      <td>Ballantine Books</td>\n",
       "      <td>http://images.amazon.com/images/P/0345413903.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0345413903.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0345413903.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578</th>\n",
       "      <td>277427</td>\n",
       "      <td>0385424736</td>\n",
       "      <td>9</td>\n",
       "      <td>The Rainmaker</td>\n",
       "      <td>John Grisham</td>\n",
       "      <td>1995</td>\n",
       "      <td>Doubleday Books</td>\n",
       "      <td>http://images.amazon.com/images/P/0385424736.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0385424736.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0385424736.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      User-ID        ISBN  Book-Rating  \\\n",
       "1456   277427  002542730X           10   \n",
       "1474   277427  0061009059            9   \n",
       "1522   277427  0316776963            8   \n",
       "1543   277427  0345413903           10   \n",
       "1578   277427  0385424736            9   \n",
       "\n",
       "                                             Book-Title         Book-Author  \\\n",
       "1456  Politically Correct Bedtime Stories: Modern Ta...   James Finn Garner   \n",
       "1474  One for the Money (Stephanie Plum Novels (Pape...     Janet Evanovich   \n",
       "1522                             Me Talk Pretty One Day       David Sedaris   \n",
       "1543                                    The Murder Book  Jonathan Kellerman   \n",
       "1578                                      The Rainmaker        John Grisham   \n",
       "\n",
       "     Year-Of-Publication                  Publisher  \\\n",
       "1456                1994  John Wiley &amp; Sons Inc   \n",
       "1474                1995                HarperTorch   \n",
       "1522                2001             Back Bay Books   \n",
       "1543                2003           Ballantine Books   \n",
       "1578                1995            Doubleday Books   \n",
       "\n",
       "                                            Image-URL-S  \\\n",
       "1456  http://images.amazon.com/images/P/002542730X.0...   \n",
       "1474  http://images.amazon.com/images/P/0061009059.0...   \n",
       "1522  http://images.amazon.com/images/P/0316776963.0...   \n",
       "1543  http://images.amazon.com/images/P/0345413903.0...   \n",
       "1578  http://images.amazon.com/images/P/0385424736.0...   \n",
       "\n",
       "                                            Image-URL-M  \\\n",
       "1456  http://images.amazon.com/images/P/002542730X.0...   \n",
       "1474  http://images.amazon.com/images/P/0061009059.0...   \n",
       "1522  http://images.amazon.com/images/P/0316776963.0...   \n",
       "1543  http://images.amazon.com/images/P/0345413903.0...   \n",
       "1578  http://images.amazon.com/images/P/0385424736.0...   \n",
       "\n",
       "                                            Image-URL-L  \n",
       "1456  http://images.amazon.com/images/P/002542730X.0...  \n",
       "1474  http://images.amazon.com/images/P/0061009059.0...  \n",
       "1522  http://images.amazon.com/images/P/0316776963.0...  \n",
       "1543  http://images.amazon.com/images/P/0345413903.0...  \n",
       "1578  http://images.amazon.com/images/P/0385424736.0...  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#so 1266 users for 1000 books. Seems like a reasonable final dataset.\n",
    "#time to make final cuts on the catalog\n",
    "final_further_reduced_catalogCut = further_reduced_catalog[\"User-ID\"].isin(book_reviewers)\n",
    "final_further_reduced_catalog = further_reduced_catalog[final_further_reduced_catalogCut]\n",
    "final_further_reduced_catalog.head(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2. Setting up collaborative filtering\n",
    "Here, I build a collaborative filtering recommendation system for the book reviews that is based on an assignment from the Stanford Machine Learning Coursera. In that case, prepared film data from IMDB was used. Interestingly, I tracked down several bugs and flaws in the code developed for the assignment and fixed them in this implementation. Will note when applicable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1266)\n"
     ]
    }
   ],
   "source": [
    "#at this point, I am going to translate all the reviews in this catalog into a simple matrix containg reviews per user.\n",
    "#i am going to use a simple numpy matrix since the remainder of my code is prepared for such a use.\n",
    "# in the future, I will try to stick only  to Pandas framework\n",
    "#titles = np.array(most_reviewedbooks[\"Book-Title\"])\n",
    "Y = np.zeros((len(titles), len(book_reviewers)))\n",
    "print(Y.shape)\n",
    "num_books = Y.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#now we go through the final catalog line by line and fill in the elements of Y.\n",
    "#this is embarrasing and I'm sure there's a more efficient way to do this, but it's not too slow computationally for this dataset\n",
    "count = 0\n",
    "for row in final_further_reduced_catalog.values:\n",
    "    title_index = np.where(titles==row[3])[0][0]\n",
    "    user_index = np.where(book_reviewers==row[0])[0][0]   \n",
    "    Y[title_index][user_index]=row[2]\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert some additional user input. I personally went through the list of books and rated some.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I assigned this book:  The Da Vinci Code              \n",
      " This rating:  1\n",
      "I assigned this book:  The Nanny Diaries: A Novel        \n",
      " This rating:  1\n",
      "I assigned this book:  Harry Potter and the Chamber of Secrets (Book 2)              \n",
      " This rating:  5\n",
      "I assigned this book:  Harry Potter and the Sorcerer's Stone (Harry Potter (Paperback))           \n",
      " This rating:  5\n",
      "I assigned this book:  Angels &amp; Demons            \n",
      " This rating:  1\n",
      "I assigned this book:  Harry Potter and the Prisoner of Azkaban (Book 3)              \n",
      " This rating:  5\n",
      "I assigned this book:  Harry Potter and the Goblet of Fire (Book 4)               \n",
      " This rating:  5\n",
      "I assigned this book:  Harry Potter and the Order of the Phoenix (Book 5)                \n",
      " This rating:  5\n",
      "I assigned this book:  The Fellowship of the Ring (The Lord of the Rings, Part 1)              \n",
      " This rating:  8\n",
      "I assigned this book:  The Golden Compass (His Dark Materials, Book 1)                      \n",
      " This rating:  8\n",
      "I assigned this book:  American Gods                                     \n",
      " This rating:  8\n",
      "I assigned this book:  The Cider House Rules                            \n",
      " This rating:  8\n",
      "I assigned this book:  1984                                 \n",
      " This rating:  9\n",
      "I assigned this book:  The Handmaid's Tale                                 \n",
      " This rating:  7\n",
      "I assigned this book:  Animal Farm                                 \n",
      " This rating:  9\n",
      "I assigned this book:  The Subtle Knife (His Dark Materials, Book 2)                      \n",
      " This rating:  7\n",
      "I assigned this book:  The Little Prince                         \n",
      " This rating:  9\n",
      "I assigned this book:  Slaughterhouse Five or the Children's Crusade: A Duty Dance With Death                           \n",
      " This rating:  10\n",
      "I assigned this book:  Neuromancer (Remembering Tomorrow)                          \n",
      " This rating:  8\n",
      "I assigned this book:  Dune (Remembering Tomorrow)                             \n",
      " This rating:  8\n",
      "I assigned this book:  Atlas Shrugged                     \n",
      " This rating:  1\n",
      "I assigned this book:  Cat's Cradle                         \n",
      " This rating:  10\n",
      "I assigned this book:  The Stranger                            \n",
      " This rating:  9\n",
      "I assigned this book:  A Game of Thrones (A Song of Ice and Fire, Book 1)                   \n",
      " This rating:  8\n"
     ]
    }
   ],
   "source": [
    "#creating an extra column for the Y matrix \n",
    "my_ratings = np.zeros(num_books)\n",
    "#opening text file. This text file has the 1000 most reviewed books. One book per row.\n",
    "#For each book I rated, I assign a numerical score at the end of the line.\n",
    "ratings_file = open('sasha_top_books.txt')\n",
    "rating_lines = ratings_file.readlines()\n",
    "index_ratings_ar = []\n",
    "ratings_ar = []\n",
    "\n",
    "#I will use regular expression to tease out the books that have been rated.\n",
    "import re\n",
    "\n",
    "for line in rating_lines:\n",
    "    #look for all lines that end with a 1-2 digit number, and then a newline character\n",
    "    rating = re.findall('\\d+', line[-3:])\n",
    "    if (len(rating)>0):\n",
    "        rating_value = int(rating[0])\n",
    "        \n",
    "        #is the rating one or two characters?\n",
    "        if (rating_value>=10):\n",
    "            stripped_line = line[0:-3]\n",
    "        else:\n",
    "            stripped_line = line[0:-2]\n",
    "        \n",
    "        #now lets get rid of the rating and only keep the title\n",
    "        extra_stripped_line = stripped_line.strip()\n",
    "        \n",
    "        #make sure this title matches one in our list. also find the index. \n",
    "        rating_index = np.where(titles==extra_stripped_line)\n",
    "        if len(rating_index[0])>0:\n",
    "            index_ratings_ar.append(rating_index[0][0])\n",
    "            ratings_ar.append(rating_value)\n",
    "            print ('I assigned this book: ', stripped_line,'\\n This rating: ',rating_value)\n",
    "\n",
    "#now assigning the reviews to our column vector\n",
    "count = 0\n",
    "while (count<len(index_ratings_ar)):\n",
    "    my_ratings[index_ratings_ar[count]]=ratings_ar[count]\n",
    "    count+=1\n",
    "    \n",
    "Y = np.column_stack((my_ratings,Y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#the algorithm uses a second matrix R to tell if a given book has been reviewed by a given user\n",
    "R = Y>0\n",
    "num_users = Y.shape[1]\n",
    "num_books = Y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#now setting some parameters for the collaborative filtering\n",
    "#original Coursera version had 10 features, but I find better fits with 20. \n",
    "#lambda is the regularization parameter. 10 seems to work well\n",
    "#prefactor is an extra weight for the random seed. \n",
    "prefactor = 1\n",
    "my_lambda = 10.0\n",
    "num_features = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#do-penalties - we don't want the recommender to get fooled by books that have a high mean score\n",
    "#after it's been reviewed by only a single user. This is an element I added to the system to improve upon the quality of \n",
    "#recommendations given\n",
    "\n",
    "do_penalties = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#normalization function  that also returns mean score of each book and total number of reviews \n",
    "#for each book. \n",
    "#the algorithm works better when the scores are normalized (distance from mean)\n",
    "def normalizeY(YM):\n",
    "    cooper = np.copy(YM)\n",
    "    themeans = []\n",
    "    thecounts = []\n",
    "    for arr in cooper:\n",
    "        cut = arr>0\n",
    "        themean = np.mean(arr[cut])\n",
    "        count = len(arr[cut])\n",
    "        themeans.append(themean)\n",
    "        thecounts.append(float(count))\n",
    "        arr[cut] = arr[cut] - themean\n",
    "    themeans = np.array(themeans)\n",
    "    thecounts = np.array(thecounts)\n",
    "    return [cooper, themeans,thecounts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean count  28.635\n"
     ]
    }
   ],
   "source": [
    "#normalize the matrix, and find mean number of reviews per book\n",
    "#books that have fewer reviews than the mean will have their score weighted \n",
    "#so that it des not deviate too far form a \"neutral\" review \n",
    "\n",
    "[normY, themeans,thecounts] = normalizeY(Y)\n",
    "medcount = np.mean(thecounts)\n",
    "print('mean count ',medcount)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 1267\n",
      "(1000, 20)\n",
      "(1267, 20)\n"
     ]
    }
   ],
   "source": [
    "#now we initialize matrices for books and users. Each element contains a score per feature.\n",
    "#The features themselves are basically free variables that will be solved for by the optimizer\n",
    "num_users = normY.shape[1]\n",
    "print(num_books,num_users)\n",
    "X= np.random.randn(num_books, num_features)*prefactor\n",
    "Theta = np.random.randn(num_users, num_features)*prefactor\n",
    "\n",
    "print(X.shape)\n",
    "print(Theta.shape)\n",
    "\n",
    "#unroll both matrices into a single array to be used by the optimizer\n",
    "intial_parameters_unrolled  = np.append(np.array(X).reshape(-1), np.array(Theta).reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#now we define the cost function and the gradient of the cost function\n",
    "#fully vectorized implementation\n",
    "def cost_function_matrix_reshape_J(params, theY, theR, num_users, num_books, num_features, the_lambda):\n",
    "    J = 0\n",
    "    #unroll the vector back into two feture matrices\n",
    "    X = params[0:(num_books*num_features)]\n",
    "    X = X.reshape(num_books, num_features)\n",
    "    Theta = params[(num_books*num_features):]\n",
    "    Theta = Theta.reshape(num_users, num_features)\n",
    "\n",
    "    #multiply the two feature matrices, and see how close they are to the target Y.\n",
    "    #cost function is the difference of squares. Only considering books that have actual ratings in Y.\n",
    "    J_sub1 = np.multiply((np.dot(X,Theta.transpose()) - theY), theR)\n",
    "    J = np.sum(np.multiply(J_sub1,J_sub1)) / 2.0\n",
    "\n",
    "    #add in regularization\n",
    "    J +=  the_lambda*np.sum(np.multiply(X,X))/2.0  + the_lambda*np.sum(np.multiply(Theta,Theta))/2.0 \n",
    "    print('current cost is ',J)\n",
    "\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#fully vectorized implimentation of cost function gradient\n",
    "def cost_function_matrix_reshape_grad(params, theY, R, num_users, num_books, num_features, the_lambda):\n",
    "\n",
    "    #unroll the vector back into two feture matrices\n",
    "    X = params[0:(num_books*num_features)]\n",
    "    X = X.reshape(num_books, num_features)\n",
    "    Theta = params[(num_books*num_features):]\n",
    "    Theta = Theta.reshape(num_users, num_features)\n",
    "    \n",
    "    #similar calculation to computing J\n",
    "    Grad_sub1 = np.dot(X,Theta.transpose())\n",
    "    Grad_sub2 = np.multiply(Grad_sub1, R)\n",
    "    Grad_sub3 = Grad_sub2 - theY\n",
    "    \n",
    "    #but now there's an extra step to compute the gradient by using the other feature matrix\n",
    "    X_grad = np.dot(Grad_sub3, Theta)\n",
    "    Theta_grad = np.dot(Grad_sub3.transpose(), X )\n",
    "    \n",
    "    #regularization step\n",
    "    X_grad += the_lambda*X\n",
    "    Theta_grad += the_lambda*Theta\n",
    "    \n",
    "    #return the results\n",
    "    return np.append(X_grad.reshape(-1), Theta_grad.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current cost is  550588.404912\n",
      "current cost is  2.25542654098e+13\n",
      "current cost is  15626279898.6\n",
      "current cost is  6853360.02231\n",
      "current cost is  232987.476623\n",
      "current cost is  387899.822322\n",
      "current cost is  155285.065142\n",
      "current cost is  140730.374131\n",
      "current cost is  111997.795399\n",
      "current cost is  196442.902716\n",
      "current cost is  86362.9668294\n",
      "current cost is  111567.186883\n",
      "current cost is  75320.6881834\n",
      "current cost is  62623.3386808\n",
      "current cost is  58646.8718187\n",
      "current cost is  51382.6619335\n",
      "current cost is  74726.3907013\n",
      "current cost is  49302.6904062\n",
      "current cost is  48002.6400847\n",
      "current cost is  44975.6530498\n",
      "current cost is  40574.8573859\n",
      "current cost is  35238.6630966\n",
      "current cost is  43361.3390472\n",
      "current cost is  33066.7916324\n",
      "current cost is  31903.0816402\n",
      "current cost is  35057.6414503\n",
      "current cost is  31626.1427182\n",
      "current cost is  31147.5961828\n",
      "current cost is  30634.9681049\n",
      "current cost is  32344.8046549\n",
      "current cost is  30336.0162975\n",
      "current cost is  30122.3061919\n",
      "current cost is  30163.5636905\n",
      "current cost is  30020.9636999\n",
      "current cost is  29864.6537759\n",
      "current cost is  29814.9585949\n",
      "current cost is  30202.821451\n",
      "current cost is  29758.2638547\n",
      "current cost is  29672.3024453\n",
      "current cost is  29639.5034827\n",
      "current cost is  29532.6393951\n",
      "current cost is  29487.7117436\n",
      "current cost is  29462.9837176\n",
      "current cost is  29372.0646029\n",
      "current cost is  29604.6357444\n",
      "current cost is  29352.3518353\n",
      "current cost is  29319.8869093\n",
      "current cost is  29297.9475301\n",
      "current cost is  29303.4505492\n",
      "current cost is  29271.5010191\n",
      "current cost is  29249.654539\n",
      "current cost is  29214.1790069\n",
      "current cost is  29195.6379644\n",
      "current cost is  29152.2134923\n",
      "current cost is  29108.9148943\n",
      "current cost is  29128.9066814\n",
      "current cost is  29090.9340221\n",
      "current cost is  29073.1177091\n",
      "current cost is  29055.3100527\n",
      "current cost is  29052.7933003\n",
      "current cost is  29045.5888463\n",
      "current cost is  29036.605478\n",
      "current cost is  29023.1130805\n",
      "current cost is  29019.0889513\n",
      "current cost is  29008.0786442\n",
      "current cost is  29001.0496573\n",
      "current cost is  28990.2852975\n",
      "current cost is  28986.3599472\n",
      "current cost is  28975.5119218\n",
      "current cost is  29002.7130056\n",
      "current cost is  28973.0657565\n",
      "current cost is  28968.8181771\n",
      "current cost is  28964.1963756\n",
      "current cost is  28990.1596524\n",
      "current cost is  28962.3632873\n",
      "current cost is  28959.3468084\n",
      "current cost is  28957.3975155\n",
      "current cost is  28949.8743615\n",
      "current cost is  28947.4388177\n",
      "current cost is  28946.8836619\n",
      "current cost is  28942.2698934\n",
      "current cost is  28935.5214585\n",
      "current cost is  28936.5750024\n",
      "current cost is  28932.3365517\n",
      "current cost is  28929.8556994\n",
      "current cost is  28926.2369876\n",
      "current cost is  28925.3387136\n",
      "current cost is  28922.6261648\n",
      "current cost is  28922.0974173\n",
      "current cost is  28919.3268283\n",
      "current cost is  28916.8696572\n",
      "current cost is  28913.3862092\n",
      "current cost is  28912.7267644\n",
      "current cost is  28910.4940963\n",
      "current cost is  28909.868595\n",
      "current cost is  28905.1123157\n",
      "current cost is  28901.6279984\n",
      "current cost is  28894.3066099\n",
      "current cost is  28894.6892592\n",
      "current cost is  28890.6868769\n",
      "current cost is  28884.7002173\n",
      "current cost is  28880.7005302\n",
      "current cost is  28901.0025539\n",
      "current cost is  28878.1647103\n",
      "current cost is  28875.0815932\n",
      "current cost is  28871.9581589\n",
      "current cost is  28871.252427\n",
      "current cost is  28870.1826012\n",
      "current cost is  28868.5194166\n",
      "current cost is  28866.133716\n",
      "current cost is  28865.6150047\n",
      "current cost is  28862.4155961\n",
      "current cost is  28861.3534869\n",
      "current cost is  28860.482269\n",
      "current cost is  28857.9504309\n",
      "current cost is  28860.1701003\n",
      "current cost is  28857.0577664\n",
      "current cost is  28856.3145726\n",
      "current cost is  28855.6162078\n",
      "current cost is  28854.6184653\n",
      "current cost is  28854.4110951\n",
      "current cost is  28854.7822606\n",
      "current cost is  28853.8828191\n",
      "current cost is  28853.1149372\n",
      "current cost is  28852.932203\n",
      "current cost is  28851.7964072\n",
      "current cost is  28850.1706804\n",
      "current cost is  28849.8269296\n",
      "current cost is  28850.348505\n",
      "current cost is  28848.9478913\n",
      "current cost is  28847.5119922\n",
      "current cost is  28846.6324428\n",
      "current cost is  28843.2721009\n",
      "current cost is  28842.5502203\n",
      "current cost is  28844.9961443\n",
      "current cost is  28840.9317986\n",
      "current cost is  28840.4878342\n",
      "current cost is  28839.9844082\n",
      "current cost is  28838.7554046\n",
      "current cost is  28838.1831149\n",
      "current cost is  28837.9499315\n",
      "current cost is  28837.871876\n",
      "current cost is  28837.5230914\n",
      "current cost is  28836.8155473\n",
      "current cost is  28836.3188396\n",
      "current cost is  28835.8046495\n",
      "current cost is  28835.5473644\n",
      "current cost is  28834.6172563\n",
      "current cost is  28834.0500953\n",
      "current cost is  28834.8740105\n",
      "current cost is  28833.8828977\n",
      "current cost is  28833.5772824\n",
      "current cost is  28832.9976843\n",
      "current cost is  28834.8503664\n",
      "current cost is  28832.7781678\n",
      "current cost is  28832.5139368\n",
      "current cost is  28832.2024917\n",
      "current cost is  28832.1315151\n",
      "current cost is  28832.0252506\n",
      "current cost is  28831.7176858\n",
      "current cost is  28831.3913748\n",
      "current cost is  28831.3672505\n",
      "current cost is  28831.0650053\n",
      "current cost is  28830.6142158\n",
      "current cost is  28831.1802578\n",
      "current cost is  28830.4738387\n",
      "current cost is  28830.2486272\n",
      "current cost is  28830.1293528\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 28830.129353\n",
      "         Iterations: 100\n",
      "         Function evaluations: 168\n",
      "         Gradient evaluations: 168\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import fmin_cg\n",
    "\n",
    "#now we run the optimizer fmin_cg to minimize the cost function.\n",
    "\n",
    "results = fmin_cg(cost_function_matrix_reshape_J,intial_parameters_unrolled,fprime=cost_function_matrix_reshape_grad,\\\n",
    "                      args=(normY,R,num_users,num_books,num_features,my_lambda), maxiter=100,disp=True,full_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking at the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " my newX (1000, 20)\n",
      "my newTheta  (1267, 20)\n"
     ]
    }
   ],
   "source": [
    "#unroll the results vector back into optimized feature matrices\n",
    "returns = results[0]\n",
    "newX = returns[:(num_books*num_features)]\n",
    "newX = newX.reshape(num_books, num_features)\n",
    "print(' my newX', newX.shape)\n",
    "newTheta = returns[(num_books*num_features):]\n",
    "newTheta = newTheta.reshape(num_users, num_features)\n",
    "print( 'my newTheta ',newTheta.shape)\n",
    "p = np.dot(newX,  newTheta.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#add the means back in\n",
    "my_predictions = p[:,0] + themeans\n",
    "#this is where i penalize scores of books that have fewer than the mean number of reviews\n",
    "cut = (thecounts<medcount)\n",
    "\n",
    "# i set it up such that if the book has no reviews, it is assigned a score of 5\n",
    "# if the book has more than the mean number of reviews, it is assigned whatever actual scores it got.\n",
    "# if a book has in between zero and n_mean reviews, it is given a linear combination of  5 and the predicted\n",
    "# score based on the distance to each\n",
    "if (do_penalties):\n",
    "    my_predictions[cut] = ((medcount-thecounts[cut])/medcount) * 5.0 + (thecounts[cut]/medcount)*my_predictions[cut]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 books to read\n",
      "Book to read:  The Two Towers (The Lord of the Rings, Part 2) \n",
      " Predicted score:  9.28424152774\n",
      "Book to read:  A Prayer for Owen Meany \n",
      " Predicted score:  9.27240493228\n",
      "Book to read:  The Murder Book \n",
      " Predicted score:  9.17792545246\n",
      "Book to read:  The Amber Spyglass (His Dark Materials, Book 3) \n",
      " Predicted score:  9.07783428088\n",
      "Book to read:  Anne of Green Gables (Anne of Green Gables Novels (Paperback)) \n",
      " Predicted score:  9.01570086438\n",
      "Book to read:  The Secret Garden \n",
      " Predicted score:  8.9782634167\n",
      "Book to read:  Me Talk Pretty One Day \n",
      " Predicted score:  8.97001382114\n",
      "Book to read:  Griffin &amp; Sabine: An Extraordinary Correspondence \n",
      " Predicted score:  8.96471021668\n",
      "Book to read:  Slaughterhouse Five or the Children's Crusade: A Duty Dance With Death \n",
      " Predicted score:  8.94734482835\n",
      "Book to read:  The Green Mile \n",
      " Predicted score:  8.9445561559\n",
      "Book to read:  The Stand: Complete and Uncut \n",
      " Predicted score:  8.89490935159\n",
      "Book to read:  The Little Prince \n",
      " Predicted score:  8.86492330635\n",
      "Book to read:  Holes (Yearling Newbery) \n",
      " Predicted score:  8.81294747282\n",
      "Book to read:  The Return of the King (The Lord of the Rings, Part 3) \n",
      " Predicted score:  8.76602699187\n",
      "Book to read:  Wuthering Heights \n",
      " Predicted score:  8.73709790338\n",
      "Book to read:  Seabiscuit: An American Legend \n",
      " Predicted score:  8.70234131657\n",
      "Book to read:  Cat &amp; Mouse (Alex Cross Novels) \n",
      " Predicted score:  8.69648614753\n",
      "Book to read:  Gone with the Wind \n",
      " Predicted score:  8.69079731129\n",
      "Book to read:  Sphere \n",
      " Predicted score:  8.63303025105\n",
      "Book to read:  Big Stone Gap: A Novel (Ballantine Reader's Circle) \n",
      " Predicted score:  8.627926288\n"
     ]
    }
   ],
   "source": [
    "#print top 20 best rated book.\n",
    "fingers = np.argsort(-1*my_predictions)\n",
    "print(\"Top 20 books to read\")\n",
    "\n",
    "for i in range(0, 20):\n",
    "    print ('Book to read: ',titles[fingers][i], '\\n Predicted score: ', my_predictions[fingers][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As a bonus, here are some books you shouldn't bother reading\n",
      "Book not to read:  The Da Vinci Code \n",
      " Predicted score:  2.71620665311\n",
      "Book not to read:  The Nanny Diaries: A Novel \n",
      " Predicted score:  2.89658501358\n",
      "Book not to read:  Angels &amp; Demons \n",
      " Predicted score:  3.50031604216\n",
      "Book not to read:  Four Blondes \n",
      " Predicted score:  4.44473565208\n",
      "Book not to read:  The Bourne Identity \n",
      " Predicted score:  4.52439115761\n",
      "Book not to read:  Wild Animus \n",
      " Predicted score:  4.9967538973\n",
      "Book not to read:  Divine Secrets of the Ya-Ya Sisterhood: A Novel \n",
      " Predicted score:  5.02056834301\n",
      "Book not to read:  Deception Point \n",
      " Predicted score:  5.02299172521\n",
      "Book not to read:  The Shelters of Stone (Earth's Children Series, No 5) \n",
      " Predicted score:  5.09066397905\n",
      "Book not to read:  The Celestine Prophecy \n",
      " Predicted score:  5.09366851548\n",
      "Book not to read:  Free \n",
      " Predicted score:  5.10478557147\n",
      "Book not to read:  The Little Friend \n",
      " Predicted score:  5.12931919405\n",
      "Book not to read:  Mr. Murder \n",
      " Predicted score:  5.18111023195\n",
      "Book not to read:  The Wedding \n",
      " Predicted score:  5.1836186736\n",
      "Book not to read:  Moo \n",
      " Predicted score:  5.2011816227\n",
      "Book not to read:  Pagan Babies \n",
      " Predicted score:  5.24385535518\n",
      "Book not to read:  Stupid White Men. Eine Abrechnung mit dem Amerika unter George W. Bush \n",
      " Predicted score:  5.24676958315\n",
      "Book not to read:  Full Speed (Janet Evanovich's Full Series) \n",
      " Predicted score:  5.24787595623\n",
      "Book not to read:  The Sky Is Falling \n",
      " Predicted score:  5.29034599236\n",
      "Book not to read:  My Gal Sunday \n",
      " Predicted score:  5.30733019096\n"
     ]
    }
   ],
   "source": [
    "fingers = np.argsort(my_predictions)\n",
    "print(\"As a bonus, here are some books you shouldn't bother reading\")\n",
    "for i in range(0, 20):\n",
    "    print ('Book not to read: ',titles[fingers][i], '\\n Predicted score: ', my_predictions[fingers][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Overall, I am pleased that this algorithm works. It (correctly) recommends books that I have already read and rated highly. If I really only wanted new books to read, I would obviously filter those out, but I kept them in as a sanity check. It recommends more Lord of the Rings book after I said I liked the first one. It also recommends the final His Dark Materials book, after I rated the first two pretty highly.\n",
    "\n",
    "On the other hand, the level of customization is not exactly top-notch.  I have tried it with various inputs, and it tends to always recommend a few specific titles. A Prayer for Owen Meany, Griffin & Sabine, Anne of Green Gables, and The Secret Garden are favorites. I suspect this sort of collaborative filtering is prone to function in this way. Most people only finish reading something that they are already invested in, and would rate highly. I think it's different than the small amount of investment that people would put in for a movie review. For a second pass at this problem, I would probably try association algorithms using decision trees instead of collaborative filtering. \n",
    "\n",
    "Nonetheless, there are probably a few solid recommendations in this list. I am actually pretty intrigued by Griffin & Sabine. This was a fun project! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
